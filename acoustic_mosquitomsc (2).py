# -*- coding: utf-8 -*-
"""Acoustic MosquitoMSC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_0c4cbObjWbRWeMu5a36Tn61dsgS2wu

#connecting with google drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""#loading neccesary libraries

"""

#importing neccesary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import librosa

"""#loading dataset"""

#loading the dataset both excel containing the metadata and the audio file
metadata=pd.read_csv("/content/drive/MyDrive/neurips_2021_zenodo_0_0_1.csv")
print(metadata.head())
audio_path="/content/drive/MyDrive/humbugdb_neurips_2021_1"
# Add new column with full path to each audio file
metadata["AudioPath"] = metadata["id"].apply(
    lambda x: os.path.join(audio_path, f"{x}.WAV")
)

print(metadata.isnull().sum())
print(metadata['AudioPath'].count())
print(metadata["sound_type"].value_counts())
print(metadata["species"].value_counts())
print(metadata["country"].value_counts())
print(metadata.dtypes)
metadata["record_datetime"]=pd.to_datetime(metadata["record_datetime"])
metadata["record_hour"]=metadata['record_datetime'].dt.hour

plt.figure(figsize=(20,8))
country_counts = metadata["country"].value_counts()
plt.pie(country_counts.values, labels=country_counts.index, autopct='%1.1f%%', startangle=90)
plt.title("Distribution of Country")
plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

# Use the required_species list defined in cell kIskn24WCsED
# If you have modified required_species in cell kIskn24WCsED to include other species,
# those species will also be plotted.
species_to_plot = 'an stephensi' , 'ae aegypti','culex quinquefasciatus','culex tarsalis','ae albopictus','an gambiae','an gambiae ss','an gambiae sl'

for selected_species in species_to_plot:
    plt.figure(figsize=(10,8))

    # Filter the metadata for the current species
    species_data = metadata[metadata["species"] == selected_species]

    # Count the occurrences of the selected species in each country
    country_counts = species_data["country"].value_counts().reset_index()
    country_counts.columns = ["country", "count"]

    # Create the bar plot and assign it to ax
    ax = sns.barplot(data=country_counts, x="country", y="count")

    for container in ax.containers:
        ax.bar_label(container)

    plt.xlabel("Country")
    plt.ylabel(f"Count of {selected_species}")
    plt.title(f"Distribution of {selected_species} by Country")
    plt.xticks(rotation=90)
    plt.show()

required_species=['an gambiae ss','culex pipiens complex','an arabiensis','an dirus', 'ma uniformis','an funestus ss','an squamosus','culex quinquefasciatus']
metadata=metadata[metadata["species"].isin(required_species)]
print(metadata["species"].value_counts())
print(metadata.head())

"""#subsampling dataset"""

subsampledata={
    'an arabiensis': 1985,
    'an gambiae ss': 737,
    'culex quinquefasciatus': 678,
    'culex pipiens complex': 545,
    'an funestus ss': 381,
    'an squamosus': 141,
    'ma uniformis': 131,
    'an dirus': 129
}
sampled_data=[]
for species,count in subsampledata.items():
  sampled_data.append(metadata[metadata["species"]==species].sample(n=count,random_state=42))
sampled_data=pd.concat(sampled_data)
print(sampled_data["species"].value_counts())
print(sampled_data.head())
print(sampled_data.isnull().sum())

"""#comfirming the audio paths exist

"""

# List all audio files in the directory
all_audio_files = os.listdir(audio_path)
print("Total audio files found in directory:", len(all_audio_files))
# Clean IDs in metadata
sampled_data['id'] = sampled_data['id'].astype(str).str.strip()

# Build lookup dictionary
file_lookup = {}
for f in all_audio_files:
    filename = os.path.splitext(f)[0]
    file_id = filename.split("_")[0]  # Extract ID before any underscore
    file_lookup[file_id] = os.path.join(audio_path, f)

# Map matched file paths
sampled_data["AudioPath"] = sampled_data["id"].map(file_lookup)
print("Audio paths assigned:", sampled_data['AudioPath'].notnull().sum(), "/", len(sampled_data))
print("Missing paths:", sampled_data['AudioPath'].isnull().sum())

"""#filling missing value on the species column"""

#fill misssing value on only when there is mosquito sound on the species column
target_sound_type="mosquito"
fill_mode_mosquito_species=metadata[metadata["sound_type"]== target_sound_type]["species"].mode()[0]
metadata.loc[metadata["sound_type"] == target_sound_type, "species"] = metadata.loc[metadata["sound_type"] == target_sound_type, "species"].fillna(fill_mode_mosquito_species)
print(metadata['species'].isnull().sum())

plt.figure(figsize=(20,8))
sns.boxplot(data=metadata,x="species",y="record_hour",palette="Set2")
plt.xlabel("species")
plt.ylabel("record_hour")
plt.title("Boxplot on record hour across species")
plt.xticks(rotation=90)
plt.show()

#EDA ON MOSQUITO SOUNDS


sampling_rates = []
signal_lengths = []

for index, row in sampled_data.iterrows():
    audio_path = row["AudioPath"]
    try:
        signal, sr = librosa.load(audio_path, sr=None)
        sampling_rates.append(sr)
        signal_lengths.append(len(signal))
    except Exception as e:
        print(f"❌ Error loading {audio_path}: {type(e).__name__} - {e}")
        sampling_rates.append(None)
        signal_lengths.append(None)

# Add the sampling rates and signal lengths to the DataFrame
sampled_data['sampling_rate'] = sampling_rates
sampled_data['signal_length'] = signal_lengths

# Calculate duration in seconds
sampled_data['duration_seconds'] = sampled_data['signal_length'] / sampled_data['sampling_rate']

# Calculate and print the minimum, maximum, and mean duration
min_duration = sampled_data['duration_seconds'].min()
max_duration = sampled_data['duration_seconds'].max()
mean_duration = sampled_data['duration_seconds'].mean()

print(f"Minimum Duration: {min_duration:.4f} seconds")
print(f"Maximum Duration: {max_duration:.4f} seconds")
print(f"Mean Duration: {mean_duration:.4f} seconds")

plt.figure(figsize=(20,12))
sns.histplot(data=sampled_data,x="duration_seconds",bins=20,kde="True")
plt.xlabel("duration_seconds")
plt.ylabel("count")
plt.title("countplot on duration_seconds")
plt.xticks(rotation=90)
plt.show()

#visual spectrogram on mosquito audio only
mosquito_data=sampled_data[sampled_data["sound_type"]=="mosquito"]
sampled_data=mosquito_data.groupby("species").apply(lambda x: x.sample(1,random_state=42).reset_index(drop=True))
mosquito_files=sampled_data["AudioPath"].tolist()
for i, row in sampled_data.iterrows():
    file_path = row['AudioPath']
    species = row['species']
    y, sr = librosa.load(file_path)
    S = librosa.stft(y)
    S_db = librosa.amplitude_to_db(abs(S))

    plt.figure(figsize=(10,5))
    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log')
    plt.colorbar(format='%+2.0f dB')
    plt.title(f"Spectrogram - Species: {species}\nFile: {file_path}")
    plt.xlabel("time")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

"""#encoding the nominal metadata

"""

#encoding  specific columns using one hot encoding
sampled_meta_data_encoded=pd.get_dummies(sampled_data[["sound_type","species","country"]])
print(sampled_meta_data_encoded.head())

"""#metadata -multi layer  perception  encoder

#class imbalance
"""

from collections import Counter
class_count=Counter(sampled_data['species'])
print(class_count)

class_labels=sorted(class_count.keys())
label2idx = {label: idx for idx, label in enumerate(class_labels)}
sampled_data["label"]=sampled_data['species'].map(label2idx)

from sklearn.utils.class_weight import compute_class_weight
import torch
import torch.nn as nn
# Define device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming metadata is a pandas DataFrame with a 'label_idx' column
classes = np.unique(sampled_data["label"])
class_weights_np = compute_class_weight(class_weight='balanced',
                                        classes=classes,
                                        y=sampled_data["label"])

# Convert to torch tensor for use with PyTorch loss functions
class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)

# Optional: print class weights
print("Class weights:", class_weights)

criterion = nn.CrossEntropyLoss(weight=class_weights)

"""#windowed version



"""

def extract_audio_windows(file_path,window_duration=0.96,step_duration=0.96,sr=8000):
  y,sr=librosa.load(file_path,sr=sr)
  window_size=int(window_duration*sr)
  step_size=int(step_duration*sr)

  windows=[]
  for  start in range(0,len(y)-window_size +1,step_size):
    chunk=y[start:start+window_size]
    windows.append(chunk)
  return windows

"""#convert each window to log-mel spectrogram"""

def audio_to_log_mel(y, sr=8000, n_mels=64, fmax=8000):
    """
    Converts raw audio to log-mel spectrogram.
    """
    try:
        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
        log_mel = librosa.power_to_db(mel_spec, ref=np.max)
        return log_mel
    except Exception as e:
        print(f"❌ Error converting to log-mel: {e}")
        return None

"""#featA convertion of audio files into spectrogram"""

# Configuration for Feat. A
window_duration = 0.96  # seconds
n_melsA = 64             # for Feat. A
target_time_framesA = 96 # per HumBugDB paper

all_windowsA = []

for idx, row in sampled_data.iterrows():
    file_path = row['AudioPath']
    species = row['species']

    try:
        windows = extract_audio_windows(file_path, window_duration=window_duration, step_duration=window_duration)
        label = row['label']  # Get label once per clip

        for win in windows:
            log_mel = audio_to_log_mel(win, sr=8000, n_mels=n_melsA)

            if log_mel is None:
                continue  # Skip problematic conversion

            # Pad or truncate time axis
            if log_mel.shape[1] < target_time_framesA:
                pad_width = target_time_framesA - log_mel.shape[1]
                log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant', constant_values=-100.0)
            elif log_mel.shape[1] > target_time_framesA:
                log_mel = log_mel[:, :target_time_framesA]

            # Store result
            all_windowsA.append({
                'log_mel': log_mel,
                'species': species,
                'label': label
            })

    except Exception as e:
        print(f"❌ Error processing {file_path}: {e}")

"""#featb convertion of audio files to spectrogram"""

# Choose configuration
window_duration = 1.92  # seconds
n_melsB = 128             # for Feat. A
target_time_framesB = 30 # per HumBugDB paper

all_windowsB = []

for idx, row in sampled_data.iterrows():
    file_path = row['AudioPath']
    species = row['species']

    try:
        windows = extract_audio_windows(file_path, window_duration=window_duration, step_duration=window_duration)
        for win in windows:
            log_mel = audio_to_log_mel(win, sr=8000, n_mels=n_melsB)

            # Pad or truncate time axis
            if log_mel.shape[1] < target_time_framesB:
                pad_width = target_time_framesB - log_mel.shape[1]
                log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant', constant_values=-100.0)
            elif log_mel.shape[1] > target_time_framesB:
                log_mel = log_mel[:, :target_time_framesB]

            all_windowsB.append({
                'log_mel': log_mel,
                'species': species,
                'label': row['label']
            })

    except Exception as e:
        print(f"❌ Error processing {file_path}: {e}")

"""#mozzBNNv2 model

"""

import torch.nn as nn
import torch.nn.functional as F

class mozzBNNv2(nn.Module):
    def __init__(self, input_shape=(1, 64, 96), num_classes=8, dropout_rate=0.3):
        super(mozzBNNv2, self).__init__()
        #4 convolutional layers
        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, padding=1)
        self.bn1= nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2= nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3= nn.BatchNorm2d(128)
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn4= nn.BatchNorm2d(256)

        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(dropout_rate)

        self._to_Linear = self.calculate_flatten_size(input_shape)
        self.fc1 = nn.Linear(self._to_Linear, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def calculate_flatten_size(self, input_shape):
        x = torch.rand(1, *input_shape)#convert 2d into 1d before feeding it in
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = F.relu(self.conv4(x))
        x = x.view(1, -1)
        return x.shape[1]

    def forward(self, x):#defines the flow of the neural network during training
        x = self.pool(F.relu(self.conv1(x)))
        x = self.dropout(x)#augmented dropout layers
        x = self.pool(F.relu(self.conv2(x)))
        x = self.dropout(x)
        x = self.pool(F.relu(self.conv3(x)))
        x = self.dropout(x)
        x = F.relu(self.conv4(x))
        x = self.dropout(x)

        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc2(x)

"""#creating a pytorch  dataset

"""

from torch.utils.data import Dataset
import torch

class LogMelWindowDataset(Dataset):
    def __init__(self, window_data, n_mels):
        self.data = window_data
        self.n_mels = n_mels

    def __len__(self):#returns the no of dataset in  the windows list
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        log_mel = item['log_mel']
        label = item['label']

        # Convert to Tensor, add channel dimension [1, n_mels, time_frames]
        log_mel_tensor = torch.tensor(log_mel, dtype=torch.float32).unsqueeze(0)

        return log_mel_tensor, label

"""#dataloader

"""

from torch.utils.data import DataLoader

datasetA = LogMelWindowDataset(all_windowsA, n_mels=n_melsA)
datasetB = LogMelWindowDataset(all_windowsB, n_mels=n_melsB)
dataloaderA = DataLoader(datasetA, batch_size=8, shuffle=True)
dataloaderB = DataLoader(datasetB, batch_size=8, shuffle=True)

"""#train and split for featA

"""

from sklearn.model_selection import train_test_split

# Split the all_windows data into training and validation sets
train_windowsA, val_windowsA = train_test_split(all_windowsA, test_size=0.25, random_state=42)


# Create datasets and loaders
train_datasetA = LogMelWindowDataset(train_windowsA, n_mels=n_melsA)
val_datasetA = LogMelWindowDataset(val_windowsA, n_mels=n_melsA)


train_loaderA = DataLoader(train_datasetA, batch_size=8, shuffle=True)
val_loaderA = DataLoader(val_datasetA, batch_size=8, shuffle=False)

"""#train and split B"""

train_windowsB, val_windowsB = train_test_split(all_windowsB, test_size=0.25, random_state=42)
#split the all_windowsB data into training and validation
train_datasetB = LogMelWindowDataset(train_windowsB, n_mels=n_melsB)
val_datasetB = LogMelWindowDataset(val_windowsB, n_mels=n_melsB)

train_loaderB = DataLoader(train_datasetB, batch_size=8, shuffle=True)
val_loaderB = DataLoader(val_datasetB, batch_size=8, shuffle=False)

"""#training loop for mozznet model

"""

import torch.optim as optim#provide various optimazation algorithm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")#use cuda cuda gpu if available else cpu

# Initialize the model for Feat. A
input_shape_A = (1, n_melsA, target_time_framesA)
model_A = mozzBNNv2(input_shape=input_shape_A, num_classes=8).to(device)

# Define loss function and optimizer for Feat. A
criterion_A = nn.CrossEntropyLoss(weight=class_weights) # Using class weights calculated earlier
optimizer_A = torch.optim.Adam(model_A.parameters(), lr=0.001)

# Initialize the model for Feat. B
input_shape_B = (1, n_melsB, target_time_framesB)
model_B = mozzBNNv2(input_shape=input_shape_B, num_classes=8).to(device)

# Define loss function and optimizer for Feat. B
criterion_B = nn.CrossEntropyLoss(weight=class_weights) # Using class weights calculated earlier
optimizer_B = torch.optim.Adam(model_B.parameters(), lr=0.001)

def train_model(model, train_loader, val_loader, epochs=15):#train model function
    for epoch in range(epochs):#loop for the 10 epoch
        model.train()#intialize the training
        running_loss, correct, total = 0.0, 0, 0#intialize tracking variables

        for inputs, labels in train_loader:#loop for train loader as it loads in batch
            inputs, labels = inputs.to(device), labels.to(device)#load inputs nad labels to device

            # Determine which optimizer and criterion to use based on the model
            if model == model_A:
                optimizer = optimizer_A
                criterion = criterion_A
            elif model == model_B:
                optimizer = optimizer_B
                criterion = criterion_B
            else:
                raise ValueError("Unknown model provided to train_model function")


            optimizer.zero_grad()#clear gradient after every batch
            outputs = model(inputs)
            loss = criterion(outputs, labels)#labels how far the model prediction is from the true model
            loss.backward()# compute gradient for each weight
            optimizer.step()# uses gradient to adjust the weights

            running_loss += loss.item()#adds current loss and total running loss
            _, predicted = torch.max(outputs, 1)#
            total += labels.size(0)#add total number so samples in a batch
            correct += (predicted == labels).sum().item()

        train_acc = correct / total
        val_acc = evaluate_model(model, val_loader)
        print(f"Epoch {epoch+1}/{epochs} Loss: {running_loss:.4f} Train Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}")

def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    return correct / total

print("Starting training for Feat. A model...")
train_model(model_A, train_loaderA, val_loaderA, epochs=15)
print("Training for Feat. A model finished.")

print("\nStarting training for Feat. B model...")
train_model(model_B, train_loaderB, val_loaderB, epochs=15)
print("Training for Feat. B model finished.")

"""#results graph"""

from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
import numpy as np
import torch.nn.functional as F

def plot_multiclass_metrics(model, dataloader, device, class_labels, model_name):
    model.eval()
    all_labels = []
    all_probabilities = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probabilities = F.softmax(outputs, dim=1)

            all_labels.extend(labels.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())

    all_labels_binarized = label_binarize(all_labels, classes=np.arange(len(class_labels)))

    # ROC AUC
    plt.figure(figsize=(10, 8))
    roc_aucs = []
    for i in range(len(class_labels)):
        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        roc_auc = auc(fpr, tpr)
        roc_aucs.append(roc_auc)
        plt.plot(fpr, tpr, label=f'Class {class_labels[i]} (AUC = {roc_auc:.2f})')

    # Calculate overall ROC AUC (macro average)
    mean_roc_auc = np.mean(roc_aucs)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model_name}')
    plt.legend(loc="lower right")
    plt.show()

    print(f"Overall ROC AUC (Macro Average) for {model_name}: {mean_roc_auc:.4f}")


    # Precision-Recall Curve
    plt.figure(figsize=(10, 8))
    pr_aucs = []
    for i in range(len(class_labels)):
        precision, recall, _ = precision_recall_curve(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        pr_auc = average_precision_score(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        pr_aucs.append(pr_auc)
        plt.plot(recall, precision, label=f'Class {class_labels[i]} (AP = {pr_auc:.2f})')

    # Calculate overall Precision-Recall AUC (macro average)
    mean_pr_auc = np.mean(pr_aucs)

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve - {model_name}')
    plt.legend(loc="lower left")
    plt.show()

    print(f"Overall Precision-Recall AUC (Macro Average) for {model_name}: {mean_pr_auc:.4f}")


# Assuming model_A, val_loaderA, model_B, val_loaderB, device, and class_labels are defined
plot_multiclass_metrics(model_A, val_loaderA, device, class_labels, "Model A")
plot_multiclass_metrics(model_B, val_loaderB, device, class_labels, "Model B")

"""#pretrained model RESNET

#DEFINE PRETRAINED MODEL RESNET
"""

from torch.utils.data import Dataset
import torch
import torchvision.transforms as transforms
import numpy as np
from PIL import Image

class ResNetWindowDataset(Dataset):
    def __init__(self, window_data):
        self.data = window_data

        # Define the complete transform pipeline
        self.transform = transforms.Compose([
            transforms.ToPILImage(),                       # Convert numpy image to PIL
            transforms.Resize((224, 224),interpolation=Image.BILINEAR),                 # Resize to match ResNet input
            transforms.ToTensor(),                         # Convert to tensor with shape [1, 224, 224]
            transforms.Normalize(mean=[0.5], std=[0.5])    # Normalize grayscale
        ])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        log_mel = item['log_mel']  # shape: (n_mels, time_frames)

        # Convert to float32 numpy array if not already
        log_mel = log_mel.astype(np.float32)

        # Apply transform pipeline
        img_tensor = self.transform(log_mel)

        label = item['label']
        return img_tensor, label

"""#intialize the resnet18

"""

import torch.nn as nn
import torchvision.models as models # Corrected import
import torch.nn.functional as F

class modify_resnet18_for_bnn(nn.Module):
    def __init__(self, num_classes=8, dropout_p=0.3, pretrained=False ):
        super(modify_resnet18_for_bnn, self).__init__()
        model = models.resnet18(pretrained=pretrained) # Corrected: Use models.resnet18

        # Modify conv1 to accept 1-channel input
        self.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=64,
            kernel_size=7,
            stride=2,
            padding=3,
            bias=False
        )
        self.bn1 = model.bn1 # Use batch normalization from the original model
        self.relu = model.relu # Use ReLU from the original model
        self.maxpool = model.maxpool # Use maxpool from the original model


        # Assuming BayesianBasicBlock is defined elsewhere (as it is in your notebook)
        # If not, you would need to define it here or import it.
        # For this fix, I'm assuming BayesianBasicBlock is correctly defined in the
        # cell above this one (cell_id: 4oBCGNxZLF7T).

        def replace_layer(original_layer, num_blocks):
            blocks = []
            for i in range(num_blocks):
                # Get original block to extract parameters
                original_block = original_layer[i]
                in_channels = original_block.conv1.in_channels
                out_channels = original_block.conv1.out_channels
                stride = original_block.conv1.stride # Get the stride. This might be an int or a tuple.

                # Handle stride based on its type
                if isinstance(stride, tuple):
                    stride = stride[0] # Take the first element if it's a tuple

                downsample = original_block.downsample if i == 0 else None # Use original downsample for the first block in the layer

                blocks.append(
                    BayesianBasicBlock(
                        in_channels,
                        out_channels,
                        stride=stride,
                        downsample=downsample,
                        dropout_p=dropout_p
                    )
                )
            return nn.Sequential(*blocks)

        self.layer1 = replace_layer(model.layer1, 2)
        self.layer2 = replace_layer(model.layer2, 2)
        self.layer3 = replace_layer(model.layer3, 2)
        self.layer4 = replace_layer(model.layer4, 2)

        # Replace the final classification head with dropout + FC
        self.avgpool = model.avgpool # Use avgpool from the original model
        self.fc = nn.Sequential(
            nn.Dropout(p=dropout_p),
            nn.Linear(model.fc.in_features, num_classes) # Use original in_features for the FC layer
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

from torchvision.models.resnet import BasicBlock
class BayesianBasicBlock(BasicBlock):
    def __init__(self, *args, dropout_p=0.3, **kwargs):
        super().__init__(*args, **kwargs)
        self.dropout = nn.Dropout(p=dropout_p)

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.dropout(out)  # Dropout inserted here

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out

def evaluate_resnet_mc_dropout(model, val_loader, mc_runs=30):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Multiple stochastic forward passes
            mc_outputs = []
            for _ in range(mc_runs):
                outputs = model(inputs)
                mc_outputs.append(outputs.unsqueeze(0))  # (1, B, C)

            mc_outputs = torch.cat(mc_outputs, dim=0)     # (T, B, C)
            mean_outputs = mc_outputs.mean(dim=0)         # (B, C)

            _, predicted = torch.max(mean_outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    return correct / total

"""#train and split ........dataloader

"""

from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split

train_windowsA, val_windowsA = train_test_split(all_windowsA, test_size=0.25, random_state=42)

train_datasetA = ResNetWindowDataset(train_windowsA)
val_datasetA = ResNetWindowDataset(val_windowsA)

train_loaderA = DataLoader(train_datasetA, batch_size=8, shuffle=True)
val_loaderA = DataLoader(val_datasetA, batch_size=8, shuffle=False)

train_windowsB, val_windowsB = train_test_split(all_windowsB, test_size=0.25, random_state=42)

train_datasetB = ResNetWindowDataset(train_windowsB)
val_datasetB = ResNetWindowDataset(val_windowsB)

train_loaderB = DataLoader(train_datasetB, batch_size=8, shuffle=True)
val_loaderB = DataLoader(val_datasetB, batch_size=8, shuffle=False)

"""#training loop for resnet pretrained model

"""

def train_resnet_model(model, train_loader, val_loader, epochs=18, lr=0.001):
    import torch.optim as optim
    import torch.nn as nn

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)


    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct, total = 0, 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = correct / total
        val_acc = evaluate_resnet_mc_dropout(model, val_loader)  # <-- Use MC Dropout eval here

        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")
# Initialize model
resnet_modelA =modify_resnet18_for_bnn(num_classes=8).to(device)
resnet_modelB =modify_resnet18_for_bnn(num_classes=8).to(device)



# Train on Feature Set B
print("\nTraining ResNet Model on Feature B...")
train_resnet_model(resnet_modelB, train_loaderB, val_loaderB, epochs=18)

"""#VGGish BNN  pretrained model

### VGGish Pretrained Model
"""

# Install necessary libraries. torch, openl3 (for potential future use or reference), and soundfile are helpful for audio processing.
!pip install torch openl3 soundfile
!pip install torchvggish
import torch# Import the main PyTorch library.
#load the weights after downloading VGGish model weights
from torchvggish import vggish # Import necessary modules from the library
vggish_mo = vggish()# Initialize the VGGish model provided by the library


import torch.nn as nn # Import the neural network module from PyTorch, which contains base classes for layers.
import torch.nn.functional as F # Import the functional module, which contains activation functions, pooling operations, etc.

# Define the VGGish model architecture as a PyTorch Module.
# This class inherits from nn.Module, which is the base class for all neural networks in PyTorch.
class VGGish(nn.Module):
    # The constructor of the VGGish class.
    # It initializes the layers of the network.
    def __init__(self):
        # Call the constructor of the parent class (nn.Module).
        super(VGGish, self).__init__()

        # feature extractions using sequence containers
        self.features = nn.Sequential(
            #block 1
            # First Conv layer
            nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True), # Apply the Rectified Linear Unit
            # Second Conv layer
            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), # Max pooling layer.

            # Block 2
            # first conv layer
            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # First Conv layer in Block 2: 64 input, 128 output.
            nn.ReLU(inplace=True),
            # second conv layer
            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Second Conv layer in Block 2: 128 input, 128 output.
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), # Max pooling layer.

            # Block 3
            #first conv layer
            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # First Conv layer in Block 3: 128 input, 256 output.
            nn.ReLU(inplace=True),
            #second convo layer
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Second Conv layer in Block 3: 256 input, 256 output.
            nn.ReLU(inplace=True),
            #third convo layer
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Third Conv layer in Block 3: 256 input, 256 output.
            nn.ReLU(inplace=True),
             #third convo layer
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Fourth Conv layer in Block 3: 256 input, 256 output.
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), # Max pooling layer.

            # Block 4
            #first conv layer
            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # First Conv layer in Block 4: 256 input, 512 output.
            nn.ReLU(inplace=True),
            #second convo layer
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Second Conv layer in Block 4: 512 input, 512 output.
            nn.ReLU(inplace=True),
            #third convo layer
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Third Conv layer in Block 4: 512 input, 512 output.
            nn.ReLU(inplace=True),
            #third convo layer
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Fourth Conv layer in Block 4: 512 input, 512 output.
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)) # Max pooling layer.
        )


        self.embeddings = nn.Sequential(
            nn.Linear(512 * 6 * 4, 4096), # First fully connected layer: maps from the flattened output of conv layers to 4096 features. You need to replace '?' with the calculated flattened size.
            nn.ReLU(inplace=True), # Apply ReLU activation.
            nn.Linear(4096, 4096), # Second fully connected layer: maps from 4096 to 4096 features.
            nn.ReLU(inplace=True), # Apply ReLU activation.
            nn.Linear(4096, 128), # Third fully connected layer: maps from 4096 to 128 features (the standard VGGish embedding size).
            nn.ReLU(inplace=True) # Apply ReLU activation.
        )

    # Define the forward pass of the model.
    # This method specifies how the input tensor is processed through the layers.
    def forward(self, x):
        x = self.features(x) # Pass the input tensor through the feature extraction (convolutional and pooling) layers.
        x = x.view(x.size(0), -1) # Flatten the output tensor from the convolutional layers into a 1D tensor while keeping the batch size. x.size(0) gets the batch size. -1 infers the remaining dimension.
        x = self.embeddings(x) # Pass the flattened tensor through the embedding (fully connected) layers.
        return x # Return the final embedding vector.

from torch.utils.data import Dataset
import torch
import numpy as np

class VGGishDatasetMSC(Dataset):
    def __init__(self, window_data):
        self.data = window_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        log_mel = item['log_mel']  # (64, 96)
        label = item['label']

        # Reshape to [1, 64, 96]
        log_mel = np.expand_dims(log_mel, axis=0) # Add channel dimension
        log_mel_tensor = torch.tensor(log_mel).float()  # [1, 64, 96]
        return log_mel_tensor, label

import torch.nn as nn

class BayesianVGGishClassifierMSC(nn.Module):
    def __init__(self, num_classes=8, dropout_p=0.3):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout_p),     # Dropout 1
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Dropout(dropout_p),     # Dropout 2
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        return self.fc(x)

def evaluate_mc_dropout(model, vggish, val_loader, mc_runs=30):
    model.train()  # ← Keep dropout ON!
    vggish.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Get MC predictions
            mc_outputs = []
            for _ in range(mc_runs):
                emb = vggish(inputs)
                logits = model(emb)
                mc_outputs.append(logits.unsqueeze(0))

            mc_outputs = torch.cat(mc_outputs, dim=0)  # [T, B, C]
            mean_output = mc_outputs.mean(dim=0)       # [B, C]

            _, predicted = torch.max(mean_output, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    acc = correct / total
    return acc

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader

# Stratified split
train_data, val_data = train_test_split(
    all_windowsA, test_size=0.2,
    stratify=[d['label'] for d in all_windowsA],
    random_state=42
)

train_dataset = VGGishDatasetMSC(train_data)
val_dataset = VGGishDatasetMSC(val_data)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)

def train_vggish_msc(vggish_model, classifier, train_loader, val_loader, epochs=15, lr=1e-4):
    vggish_model.eval()  # Freeze VGGish
    classifier.train()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Move models to the device
    vggish_model.to(device)
    classifier.to(device)

    # Explicitly move VGGish postprocessing parameters to the device
    if hasattr(vggish_model, 'pproc') and vggish_model.pproc is not None:
        if hasattr(vggish_model.pproc, '_pca_matrix') and vggish_model.pproc._pca_matrix is not None:
            vggish_model.pproc._pca_matrix = vggish_model.pproc._pca_matrix.to(device)
        if hasattr(vggish_model.pproc, '_pca_means') and vggish_model.pproc._pca_means is not None:
             vggish_model.pproc._pca_means = vggish_model.pproc._pca_means.to(device)


    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        running_loss, correct, total = 0.0, 0, 0
        classifier.train()

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            with torch.no_grad():
                embeddings = vggish_model(inputs)  # [B, 128]

            outputs = classifier(embeddings)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = correct / total

        # Validation
       # Use MC Dropout during validation
        val_acc = evaluate_mc_dropout(classifier, vggish_model, val_loader)


        print(f"Epoch {epoch+1}/{epochs} | Loss: {running_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}")
from torchvggish import vggish

vggish_model = vggish()  # Pretrained VGGish
classifier = BayesianVGGishClassifierMSC(num_classes=8)

train_vggish_msc(vggish_model, classifier, train_loader, val_loader)

from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
import numpy as np
import torch.nn.functional as F

def plot_multiclass_metrics(model, dataloader, device, class_labels, model_name):
    model.eval()
    all_labels = []
    all_probabilities = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probabilities = F.softmax(outputs, dim=1)

            all_labels.extend(labels.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())

    all_labels_binarized = label_binarize(all_labels, classes=np.arange(len(class_labels)))

    # ROC AUC
    plt.figure(figsize=(10, 8))
    roc_aucs = []
    for i in range(len(class_labels)):
        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        roc_auc = auc(fpr, tpr)
        roc_aucs.append(roc_auc)
        plt.plot(fpr, tpr, label=f'Class {class_labels[i]} (AUC = {roc_auc:.2f})')

    # Calculate overall ROC AUC (macro average)
    mean_roc_auc = np.mean(roc_aucs)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model_name}')
    plt.legend(loc="lower right")
    plt.show()

    print(f"Overall ROC AUC (Macro Average) for {model_name}: {mean_roc_auc:.4f}")


    # Precision-Recall Curve
    plt.figure(figsize=(10, 8))
    pr_aucs = []
    for i in range(len(class_labels)):
        precision, recall, _ = precision_recall_curve(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        pr_auc = average_precision_score(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        pr_aucs.append(pr_auc)
        plt.plot(recall, precision, label=f'Class {class_labels[i]} (AP = {pr_auc:.2f})')

    # Calculate overall Precision-Recall AUC (macro average)
    mean_pr_auc = np.mean(pr_aucs)

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve - {model_name}')
    plt.legend(loc="lower left")
    plt.show()

    print(f"Overall Precision-Recall AUC (Macro Average) for {model_name}: {mean_pr_auc:.4f}")


# Assuming model_A, val_loaderA, model_B, val_loaderB, device, and class_labels are defined
plot_multiclass_metrics(vggish_model, val_loader, device, class_labels, "vggish_model")