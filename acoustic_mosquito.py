# -*- coding: utf-8 -*-
"""Acoustic Mosquito.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_0c4cbObjWbRWeMu5a36Tn61dsgS2wu

#connecting with google drive
"""

from google.colab import drive
drive.mount("/content/drive")

"""#loading neccesary libraries

"""

#importing neccesary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import librosa

"""#loading dataset"""

#loading the dataset both excel containing the metadata and the audio file
metadata=pd.read_csv("/content/drive/MyDrive/neurips_2021_zenodo_0_0_1.csv")
print(metadata.head())
audio_path="/content/drive/MyDrive/humbugdb_neurips_2021_1"
# Add new column with full path to each audio file
metadata["AudioPath"] = metadata["id"].apply(
    lambda x: os.path.join(audio_path, f"{x}.WAV")
)

print(metadata.isnull().sum())
print(metadata['AudioPath'].count())
print(metadata["sound_type"].value_counts())
print(metadata["species"].value_counts())
print(metadata["country"].value_counts())
print(metadata.dtypes)
metadata["record_datetime"]=pd.to_datetime(metadata["record_datetime"])
metadata["record_hour"]=metadata['record_datetime'].dt.hour
required_species=['an gambiae ss','culex pipiens complex','an arabiensis','an dirus', 'ma uniformis','an funestus ss','an squamosus','culex quinquefasciatus']
metadata=metadata[metadata["species"].isin(required_species)]
print(metadata["species"].value_counts())
print(metadata.head())

"""#subsampling dataset"""

subsampledata={
    'an arabiensis': 1985,
    'an gambiae ss': 737,
    'culex quinquefasciatus': 678,
    'culex pipiens complex': 545,
    'an funestus ss': 381,
    'an squamosus': 141,
    'ma uniformis': 131,
    'an dirus': 129
}
sampled_data=[]
for species,count in subsampledata.items():
  sampled_data.append(metadata[metadata["species"]==species].sample(n=count,random_state=42))
sampled_data=pd.concat(sampled_data)
print(sampled_data["species"].value_counts())
print(sampled_data.head())
print(sampled_data.isnull().sum())

"""#comfirming the audio paths exist

"""

# List all audio files in the directory
all_audio_files = os.listdir(audio_path)
print("Total audio files found in directory:", len(all_audio_files))
# Clean IDs in metadata
sampled_data['id'] = sampled_data['id'].astype(str).str.strip()

# Build lookup dictionary
file_lookup = {}
for f in all_audio_files:
    filename = os.path.splitext(f)[0]
    file_id = filename.split("_")[0]  # Extract ID before any underscore
    file_lookup[file_id] = os.path.join(audio_path, f)

# Map matched file paths
sampled_data["AudioPath"] = sampled_data["id"].map(file_lookup)
print("Audio paths assigned:", sampled_data['AudioPath'].notnull().sum(), "/", len(sampled_data))
print("Missing paths:", sampled_data['AudioPath'].isnull().sum())

"""#filling missing value on the species column"""

#fill misssing value on only when there is mosquito sound on the species column
target_sound_type="mosquito"
fill_mode_mosquito_species=metadata[metadata["sound_type"]== target_sound_type]["species"].mode()[0]
metadata.loc[metadata["sound_type"] == target_sound_type, "species"] = metadata.loc[metadata["sound_type"] == target_sound_type, "species"].fillna(fill_mode_mosquito_species)
print(metadata['species'].isnull().sum())

plt.figure(figsize=(20,8))
sns.boxplot(data=metadata,x="species",y="record_hour",palette="Set2")
plt.xlabel("species")
plt.ylabel("record_hour")
plt.title("Boxplot on record hour across species")
plt.xticks(rotation=90)
plt.show()

#EDA ON MOSQUITO SOUNDS


sampling_rates = []
signal_lengths = []

for index, row in sampled_data.iterrows():
    audio_path = row["AudioPath"]
    try:
        signal, sr = librosa.load(audio_path, sr=None)
        sampling_rates.append(sr)
        signal_lengths.append(len(signal))
    except Exception as e:
        print(f"❌ Error loading {audio_path}: {type(e).__name__} - {e}")
        sampling_rates.append(None)
        signal_lengths.append(None)

# Add the sampling rates and signal lengths to the DataFrame
sampled_data['sampling_rate'] = sampling_rates
sampled_data['signal_length'] = signal_lengths

# Calculate duration in seconds
sampled_data['duration_seconds'] = sampled_data['signal_length'] / sampled_data['sampling_rate']

# Calculate and print the minimum, maximum, and mean duration
min_duration = sampled_data['duration_seconds'].min()
max_duration = sampled_data['duration_seconds'].max()
mean_duration = sampled_data['duration_seconds'].mean()

print(f"Minimum Duration: {min_duration:.4f} seconds")
print(f"Maximum Duration: {max_duration:.4f} seconds")
print(f"Mean Duration: {mean_duration:.4f} seconds")

plt.figure(figsize=(20,12))
sns.histplot(data=sampled_data,x="duration_seconds",bins=20,kde="True")
plt.xlabel("duration_seconds")
plt.ylabel("count")
plt.title("countplot on duration_seconds")
plt.xticks(rotation=90)
plt.show()

#visual spectrogram on mosquito audio only
mosquito_data=sampled_data[sampled_data["sound_type"]=="mosquito"]
sampled_data=mosquito_data.groupby("species").apply(lambda x: x.sample(1,random_state=42).reset_index(drop=True))
mosquito_files=sampled_data["AudioPath"].tolist()
for i, row in sampled_data.iterrows():
    file_path = row['AudioPath']
    species = row['species']
    y, sr = librosa.load(file_path)
    S = librosa.stft(y)
    S_db = librosa.amplitude_to_db(abs(S))

    plt.figure(figsize=(10,5))
    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log')
    plt.colorbar(format='%+2.0f dB')
    plt.title(f"Spectrogram - Species: {species}\nFile: {file_path}")
    plt.xlabel("time")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

"""#encoding the nominal metadata

"""

#encoding  specific columns using one hot encoding
sampled_meta_data_encoded=pd.get_dummies(sampled_data[["sound_type","species","country"]])
print(sampled_meta_data_encoded.head())

"""#metadata -multi layer  perception  encoder

#class imbalance
"""

from collections import Counter
class_count=Counter(sampled_data['species'])
print(class_count)

class_labels=sorted(class_count.keys())
label2idx = {label: idx for idx, label in enumerate(class_labels)}
sampled_data["label"]=sampled_data['species'].map(label2idx)

from sklearn.utils.class_weight import compute_class_weight
import torch
import torch.nn as nn
# Define device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Assuming metadata is a pandas DataFrame with a 'label_idx' column
classes = np.unique(sampled_data["label"])
class_weights_np = compute_class_weight(class_weight='balanced',
                                        classes=classes,
                                        y=sampled_data["label"])

# Convert to torch tensor for use with PyTorch loss functions
class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)

# Optional: print class weights
print("Class weights:", class_weights)

criterion = nn.CrossEntropyLoss(weight=class_weights)

"""#windowed version



"""

def extract_audio_windows(file_path,window_duration=0.96,step_duration=0.96,sr=8000):
  y,sr=librosa.load(file_path,sr=sr)
  window_size=int(window_duration*sr)
  step_size=int(step_duration*sr)

  windows=[]
  for  start in range(0,len(y)-window_size +1,step_size):
    chunk=y[start:start+window_size]
    windows.append(chunk)
  return windows

"""#convert each window to log-mel spectrogram"""

def audio_to_log_mel(y, sr=8000, n_mels=64, fmax=8000):
    """
    Converts raw audio to log-mel spectrogram.
    """
    try:
        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)
        log_mel = librosa.power_to_db(mel_spec, ref=np.max)
        return log_mel
    except Exception as e:
        print(f"❌ Error converting to log-mel: {e}")
        return None

"""#featA convertion of audio files into spectrogram"""

# Configuration for Feat. A
window_duration = 0.96  # seconds
n_mels = 64             # for Feat. A
target_time_frames = 96 # per HumBugDB paper

all_windows = []

for idx, row in sampled_data.iterrows():
    file_path = row['AudioPath']
    species = row['species']

    try:
        windows = extract_audio_windows(file_path, window_duration=window_duration, step_duration=window_duration)
        label = row['label']  # Get label once per clip

        for win in windows:
            log_mel = audio_to_log_mel(win, sr=8000, n_mels=n_mels)

            if log_mel is None:
                continue  # Skip problematic conversion

            # Pad or truncate time axis
            if log_mel.shape[1] < target_time_frames:
                pad_width = target_time_frames - log_mel.shape[1]
                log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant', constant_values=-100.0)
            elif log_mel.shape[1] > target_time_frames:
                log_mel = log_mel[:, :target_time_frames]

            # Store result
            all_windows.append({
                'log_mel': log_mel,
                'species': species,
                'label': label
            })

    except Exception as e:
        print(f"❌ Error processing {file_path}: {e}")

"""#featb convertion of audio files to spectrogram"""

# Choose configuration
window_duration = 1.92  # seconds
n_mels = 128             # for Feat. A
target_time_frames = 30 # per HumBugDB paper

all_windows = []

for idx, row in sampled_data.iterrows():
    file_path = row['AudioPath']
    species = row['species']

    try:
        windows = extract_audio_windows(file_path, window_duration=window_duration, step_duration=window_duration)
        for win in windows:
            log_mel = audio_to_log_mel(win, sr=8000, n_mels=n_mels)

            # Pad or truncate time axis
            if log_mel.shape[1] < target_time_frames:
                pad_width = target_time_frames - log_mel.shape[1]
                log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant', constant_values=-100.0)
            elif log_mel.shape[1] > target_time_frames:
                log_mel = log_mel[:, :target_time_frames]

            all_windows.append({
                'log_mel': log_mel,
                'species': species,
                'label': row['label']
            })

    except Exception as e:
        print(f"❌ Error processing {file_path}: {e}")

"""#mozzBNNv2 model

"""

import torch.nn as nn
import torch.nn.functional as F

class mozzBNNv2(nn.Module):
    def __init__(self, input_shape=(1, 64, 96), num_classes=8, dropout_rate=0.3):
        super(mozzBNNv2, self).__init__()
        #4 convolutional layers
        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)

        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(dropout_rate)

        self._to_Linear = self.calculate_flatten_size(input_shape)
        self.fc1 = nn.Linear(self._to_Linear, 512)
        self.fc2 = nn.Linear(512, num_classes)

    def calculate_flatten_size(self, input_shape):
        x = torch.rand(1, *input_shape)
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = F.relu(self.conv4(x))
        x = x.view(1, -1)
        return x.shape[1]

    def forward(self, x):#defines the flow of the neural network during training
        x = self.pool(F.relu(self.conv1(x)))
        x = self.dropout(x)
        x = self.pool(F.relu(self.conv2(x)))
        x = self.dropout(x)
        x = self.pool(F.relu(self.conv3(x)))
        x = self.dropout(x)
        x = F.relu(self.conv4(x))
        x = self.dropout(x)

        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc2(x)

"""#creating a pytorch  dataset

"""

from torch.utils.data import Dataset
import torch

class LogMelWindowDataset(Dataset):
    def __init__(self, window_data, n_mels):
        self.data = window_data
        self.n_mels = n_mels

    def __len__(self):#returns the no of dataset in  the windows list
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        log_mel = item['log_mel']
        label = item['label']

        # Convert to Tensor, add channel dimension [1, n_mels, time_frames]
        log_mel_tensor = torch.tensor(log_mel, dtype=torch.float32).unsqueeze(0)

        return log_mel_tensor, label

"""#dataloader

"""

from torch.utils.data import DataLoader

dataset = LogMelWindowDataset(all_windows, n_mels=n_mels)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

"""#train and split

"""

from sklearn.model_selection import train_test_split

# Split the all_windows data into training and validation sets
train_windows, val_windows = train_test_split(all_windows, test_size=0.25, random_state=42)


# Create datasets and loaders
train_dataset = LogMelWindowDataset(train_windows, n_mels=n_mels)
val_dataset = LogMelWindowDataset(val_windows, n_mels=n_mels)


train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

"""#training loop for mozznet model

"""

import torch.optim as optim#provide various optimazation algorithm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")#use cuda cuda gpu if available else cpu

input_shape = (1, n_mels, target_time_frames)#intialize the model
model = mozzBNNv2(input_shape=input_shape, num_classes=8).to(device)#move the model to device

criterion = nn.CrossEntropyLoss()#compare model output vs true label
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)#update model weigths based on gradiet

def train_model(model, train_loader, val_loader, epochs=10):#train model function
    for epoch in range(epochs):#loop for the 10 epoch
        model.train()#intialize the training
        running_loss, correct, total = 0.0, 0, 0#intialize tracking variables

        for inputs, labels in train_loader:#loop for train loader as it loads in batch
            inputs, labels = inputs.to(device), labels.to(device)#load inputs nad labels to device

            optimizer.zero_grad()#clear gradient after every batch
            outputs = model(inputs)
            loss = criterion(outputs, labels)#labels how far the model prediction is from the true model
            loss.backward()# compute gradient for each weight
            optimizer.step()# uses gradient to adjust the weights

            running_loss += loss.item()#adds current loss and total running loss
            _, predicted = torch.max(outputs, 1)#
            total += labels.size(0)#add total number so samples in a batch
            correct += (predicted == labels).sum().item()

        train_acc = correct / total
        val_acc = evaluate_model(model, val_loader)
        print(f"Epoch {epoch+1}/{epochs} Loss: {running_loss:.4f} Train Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}")

def evaluate_model(model, val_loader):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    return correct / total

train_model(model, train_loader, val_loader, epochs=10)

from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
import numpy as np

def plot_multiclass_metrics(model, dataloader, device, class_labels):
    model.eval()
    all_labels = []
    all_probabilities = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probabilities = F.softmax(outputs, dim=1)

            all_labels.extend(labels.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())

    all_labels_binarized = label_binarize(all_labels, classes=np.arange(len(class_labels)))

    # ROC AUC
    plt.figure(figsize=(10, 8))
    roc_aucs = []
    for i in range(len(class_labels)):
        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        roc_auc = auc(fpr, tpr)
        roc_aucs.append(roc_auc)
        plt.plot(fpr, tpr, label=f'Class {class_labels[i]} (AUC = {roc_auc:.2f})')

    # Calculate overall ROC AUC (macro average)
    mean_roc_auc = np.mean(roc_aucs)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - mozzBNNv2')
    plt.legend(loc="lower right")
    plt.show()

    print(f"Overall ROC AUC (Macro Average): {mean_roc_auc:.4f}")


    # Precision-Recall Curve
    plt.figure(figsize=(10, 8))
    pr_aucs = []
    for i in range(len(class_labels)):
        precision, recall, _ = precision_recall_curve(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        pr_auc = average_precision_score(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        pr_aucs.append(pr_auc)
        plt.plot(recall, precision, label=f'Class {class_labels[i]} (AP = {pr_auc:.2f})')

    # Calculate overall Precision-Recall AUC (macro average)
    mean_pr_auc = np.mean(pr_aucs)

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve - mozzBNNv2')
    plt.legend(loc="lower left")
    plt.show()

    print(f"Overall Precision-Recall AUC (Macro Average): {mean_pr_auc:.4f}")


plot_multiclass_metrics(model, val_loader, device, class_labels)

import pickle
with open("model.pkl","wb") as f:
  pickle.dump(model,f)

"""#pretrained model RESNET

#DEFINE PRETRAINED MODEL RESNET
"""

from torch.utils.data import Dataset
import torch
import torchvision.transforms as transforms
import numpy as np # Import numpy
class ResNetWindowDataset(Dataset):
    def __init__(self, window_data):
        self.data = window_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        log_mel = item['log_mel']  # (n_mels, time_frames)

        # Transpose to (H, W) → (time_frames, n_mels)
        log_mel = np.transpose(log_mel)  # Now shape: (time_frames, n_mels)

        # Convert to 2D grayscale image
        img = torch.tensor(log_mel, dtype=torch.float32)

        # Apply transforms (ToPILImage expects (H, W))
        img_pil = transforms.ToPILImage()(img.numpy())
        img_resized = transforms.Resize((224, 224))(img_pil)
        img_tensor = transforms.ToTensor()(img_resized)  # Shape: [1, 224, 224]

        label = item['label']
        return img_tensor, label

"""#transform"""

import torch.nn as nn
import torchvision.models as models

class ResNetSpectrogram(nn.Module):
    def __init__(self, num_classes=8, pretrained=True):
        super(ResNetSpectrogram, self).__init__()

        self.model = models.resnet18(pretrained=pretrained)

        # Modify first Conv layer to accept 1-channel inputs
        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

        # Final FC layer for species classification
        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)

    def forward(self, x):
        return self.model(x)

from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split

train_windows, val_windows = train_test_split(all_windows, test_size=0.25, random_state=42)

train_dataset = ResNetWindowDataset(train_windows)
val_dataset = ResNetWindowDataset(val_windows)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

"""#training loop for resnet pretrained model

"""

import torch.optim as optim
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
resnet_model = ResNetSpectrogram(num_classes=8).to(device) # Use 8 for the number of species

# Define loss function and optimizer for RESNETaudio
criterion_resnet = nn.CrossEntropyLoss()
optimizer_resnet = optim.Adam(resnet_model.parameters(), lr=0.001)

# Training and evaluation functions for RESNETaudio
def train_resnet_model(model, train_loader, val_loader, epochs=10):
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct, total = 0, 0

        for inputs, labels in train_loader:
            # Apply the transformation to the inputs # Remove this line and the following list comprehension
            # inputs = torch.stack([resnet_transform(input.squeeze(0).cpu().numpy()) for input in inputs]).to(device)
            inputs, labels = inputs.to(device), labels.to(device)


            optimizer_resnet.zero_grad()
            outputs = model(inputs)
            loss = criterion_resnet(outputs, labels)
            loss.backward()
            optimizer_resnet.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = correct / total
        val_acc = evaluate_resnet_model(model, val_loader)

        print(f"RESNET Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

def evaluate_resnet_model(model, val_loader):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            # Apply the transformation to the inputs # Remove this line and the following list comprehension
            # inputs = torch.stack([resnet_transform(input.squeeze(0).cpu().numpy()) for input in inputs]).to(device)
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    return correct / total


train_resnet_model(resnet_model, train_loader, val_loader, epochs=10)

"""#VGGish BNN  pretrained model

### VGGish Pretrained Model
"""

# Install necessary libraries. torch, openl3 (for potential future use or reference), and soundfile are helpful for audio processing.
!pip install torch openl3 soundfile
!pip install torchvggish
import torch# Import the main PyTorch library.
#load the weights after downloading VGGish model weights
from torchvggish import vggish # Import necessary modules from the library
vggish_mo = vggish()# Initialize the VGGish model provided by the library


import torch.nn as nn # Import the neural network module from PyTorch, which contains base classes for layers.
import torch.nn.functional as F # Import the functional module, which contains activation functions, pooling operations, etc.

# Define the VGGish model architecture as a PyTorch Module.
# This class inherits from nn.Module, which is the base class for all neural networks in PyTorch.
class VGGish(nn.Module):
    # The constructor of the VGGish class.
    # It initializes the layers of the network.
    def __init__(self):
        # Call the constructor of the parent class (nn.Module).
        super(VGGish, self).__init__()

        # feature extractions using sequence containers
        self.features = nn.Sequential(
            #block 1
            # First Conv layer
            nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.ReLU(inplace=True), # Apply the Rectified Linear Unit
            # Second Conv layer
            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), # Max pooling layer.

            # Block 2
            # first conv layer
            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # First Conv layer in Block 2: 64 input, 128 output.
            nn.ReLU(inplace=True),
            # second conv layer
            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Second Conv layer in Block 2: 128 input, 128 output.
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), # Max pooling layer.

            # Block 3
            #first conv layer
            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # First Conv layer in Block 3: 128 input, 256 output.
            nn.ReLU(inplace=True),
            #second convo layer
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Second Conv layer in Block 3: 256 input, 256 output.
            nn.ReLU(inplace=True),
            #third convo layer
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Third Conv layer in Block 3: 256 input, 256 output.
            nn.ReLU(inplace=True),
             #third convo layer
            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Fourth Conv layer in Block 3: 256 input, 256 output.
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), # Max pooling layer.

            # Block 4
            #first conv layer
            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # First Conv layer in Block 4: 256 input, 512 output.
            nn.ReLU(inplace=True),
            #second convo layer
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Second Conv layer in Block 4: 512 input, 512 output.
            nn.ReLU(inplace=True),
            #third convo layer
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Third Conv layer in Block 4: 512 input, 512 output.
            nn.ReLU(inplace=True),
            #third convo layer
            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), # Fourth Conv layer in Block 4: 512 input, 512 output.
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)) # Max pooling layer.
        )


        self.embeddings = nn.Sequential(
            nn.Linear(512 * 6 * 4, 4096), # First fully connected layer: maps from the flattened output of conv layers to 4096 features. You need to replace '?' with the calculated flattened size.
            nn.ReLU(inplace=True), # Apply ReLU activation.
            nn.Linear(4096, 4096), # Second fully connected layer: maps from 4096 to 4096 features.
            nn.ReLU(inplace=True), # Apply ReLU activation.
            nn.Linear(4096, 128), # Third fully connected layer: maps from 4096 to 128 features (the standard VGGish embedding size).
            nn.ReLU(inplace=True) # Apply ReLU activation.
        )

    # Define the forward pass of the model.
    # This method specifies how the input tensor is processed through the layers.
    def forward(self, x):
        x = self.features(x) # Pass the input tensor through the feature extraction (convolutional and pooling) layers.
        x = x.view(x.size(0), -1) # Flatten the output tensor from the convolutional layers into a 1D tensor while keeping the batch size. x.size(0) gets the batch size. -1 infers the remaining dimension.
        x = self.embeddings(x) # Pass the flattened tensor through the embedding (fully connected) layers.
        return x # Return the final embedding vector.

from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
import numpy as np

def plot_multiclass_metrics(model, dataloader, device, class_labels):
    model.eval()
    all_labels = []
    all_probabilities = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probabilities = F.softmax(outputs, dim=1)

            all_labels.extend(labels.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())

    all_labels_binarized = label_binarize(all_labels, classes=np.arange(len(class_labels)))

    # ROC AUC
    plt.figure(figsize=(10, 8))
    roc_aucs = []
    for i in range(len(class_labels)):
        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        roc_auc = auc(fpr, tpr)
        roc_aucs.append(roc_auc)
        plt.plot(fpr, tpr, label=f'Class {class_labels[i]} (AUC = {roc_auc:.2f})')

    # Calculate overall ROC AUC (macro average)
    mean_roc_auc = np.mean(roc_aucs)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - mozzBNNv2')
    plt.legend(loc="lower right")
    plt.show()

    print(f"Overall ROC AUC (Macro Average): {mean_roc_auc:.4f}")


    # Precision-Recall Curve
    plt.figure(figsize=(10, 8))
    pr_aucs = []
    for i in range(len(class_labels)):
        precision, recall, _ = precision_recall_curve(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        pr_auc = average_precision_score(all_labels_binarized[:, i], np.array(all_probabilities)[:, i])
        pr_aucs.append(pr_auc)
        plt.plot(recall, precision, label=f'Class {class_labels[i]} (AP = {pr_auc:.2f})')

    # Calculate overall Precision-Recall AUC (macro average)
    mean_pr_auc = np.mean(pr_aucs)

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve - mozzBNNv2')
    plt.legend(loc="lower left")
    plt.show()

    print(f"Overall Precision-Recall AUC (Macro Average): {mean_pr_auc:.4f}")


plot_multiclass_metrics(model, val_loader, device, class_labels)